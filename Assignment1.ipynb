{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI/C8KAFq8nWg60RHiKYYP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyash007/deep_learning_course/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 1 CS6910**\n",
        "Shreyash Gadgil (ED22S016)"
      ],
      "metadata": {
        "id": "9LINEVNo0Fo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login aa5afea12b4fda1e7f8310b597eb17c73d1176d2 #my API key for wandb login "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC2z2tS8Ccxo",
        "outputId": "c30edd29-8dd5-4e4b-ebc0-1c512aa97d2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.16.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "G4iCjM7Q0SiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P0c3goK0vrQL"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import wandb\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1 Solution**\n",
        "Labelling dataset"
      ],
      "metadata": {
        "id": "a0Q5Mjvy0XZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "#importing dataset(add this in show_images function as variable)\n",
        "\n",
        "#checking size of training and test dataset\n",
        "print(\"Training dataset shape, X=\",X_train.shape,\", Y=\",Y_train.shape)\n",
        "print(\"Test dataset shape, X=\",X_test.shape,\", Y=\",Y_test.shape)\n",
        "\n",
        "n_img_classes=len(np.unique(Y_train)) #to find out number of unique images \n",
        "img_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] #taken from keras documentation\n",
        "\n",
        "\n",
        "def show_images(n_img_classes,img_labels,X,Y):\n",
        "\n",
        "   setImgs = []\n",
        "   setLabels=[]\n",
        "\n",
        "   for i in range(len(X)):\n",
        "     if len(setImgs)==n_img_classes:\n",
        "       break\n",
        "     if img_labels[Y[i]] not in setLabels:\n",
        "      setImgs.append(X[i])\n",
        "      setLabels.append(img_labels[Y[i]])\n",
        "\n",
        "   run=wandb.init(project='CS-6910 A1',entity='shreyashgadgil007',reinit=True)\n",
        "   run.log({\"Sample from each class\":list(wandb.Image(img,caption= caption) for img,caption in zip(setImgs,setLabels))})\n",
        "   run.finish()\n",
        "\n",
        "#--------------------------------------------------------------------------Q1 SOLUTION-----------------------------------------------------------------------------------------------\n",
        "#show_images(n_img_classes,img_labels,X_test,Y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPtktkP0cLG",
        "outputId": "970f9bec-9ca1-41ae-9592-318e03d29cd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset shape, X= (60000, 28, 28) , Y= (60000,)\n",
            "Test dataset shape, X= (10000, 28, 28) , Y= (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing dataset for operations**\n",
        "1. Creating training,test,validation split for dataset\n",
        "2. Reshaping image matrix and normalizing pixel values\n",
        "3. One hot encoding classifications\n"
      ],
      "metadata": {
        "id": "ySDC6Ck0GjBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess():\n",
        "  #change the code below to accept different dataset\n",
        "  (X_1, Y_1), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "  \n",
        "  #importing dataset again and normalizing\n",
        "  X_1 = X_1.reshape(X_1.shape[0],-1)/255.0\n",
        "  X_test = X_test.reshape(X_test.shape[0],-1)/255.0\n",
        "\n",
        "  #training and validation split as specified in the question 10%\n",
        "  X_train, X_val, Y_train, Y_val= train_test_split(X_1,Y_1,test_size=0.1,random_state=0)\n",
        "  \n",
        "  #one hot encoding\n",
        "  Y_train_encoded=np.zeros((Y_train.shape[0],10),dtype=np.int32)\n",
        "  for i in range(len(Y_train)):\n",
        "    Y_train_encoded[i][Y_train[i]]=1\n",
        "\n",
        "  Y_val_encoded=np.zeros((Y_val.shape[0],10),dtype=np.int32)\n",
        "  for i in range(len(Y_val)):\n",
        "    Y_val_encoded[i][Y_val[i]]=1\n",
        "\n",
        "  Y_test_encoded=np.zeros((Y_test.shape[0],10),dtype=np.int32)\n",
        "  for i in range(len(Y_test)):\n",
        "    Y_test_encoded[i][Y_test[i]]=1\n",
        "\n",
        "  return X_train.T,X_test.T,X_val.T,Y_train.T,Y_val.T,Y_test.T,Y_train_encoded.T,Y_val_encoded.T,Y_test_encoded.T\n",
        "\n",
        "#Below variables will be used in the entire code\n",
        "X_train,X_test,X_val,Y_train,Y_val,Y_test,Y_train_encoded,Y_val_encoded,Y_test_encoded=dataset_preprocess()\n",
        "print(Y_val_encoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuMSSFIfGiZ_",
        "outputId": "3cc2ac6c-bd94-4828-e2ce-e4695972c463"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper functions**\n",
        "1. Declared activation functions\n",
        "2. Declared initialisation functions\n",
        "3. Declared a function to initialize weights and biases"
      ],
      "metadata": {
        "id": "XCmVKBiE362R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------ACTIVATION FUNCTIONS AND THEIR GRADIENTS-------------------------------------------------------------------------------\n",
        "def relu(X):\n",
        "  return np.maximum(0,X)\n",
        "\n",
        "def grad_relu(X):\n",
        "  return X>0\n",
        "\n",
        "def sigmoid(X):\n",
        "  return 1/(1+np.exp(-X))\n",
        "\n",
        "def grad_sigmoid(X):\n",
        "  return (sigmoid(X))*(1-sigmoid(X))\n",
        "\n",
        "def tanh(X):\n",
        "  return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "\n",
        "def grad_tanh(X):\n",
        "  return 1-(tanh(X))**2\n",
        "\n",
        "def softmax(X):\n",
        "  # Calculates the softmax function\n",
        "  e_X = np.exp(X - np.max(X, axis = 0))\n",
        "  return e_X / e_X.sum(axis = 0)\n",
        "\n",
        "def softmax_cross_entropy(Y,Y_pred):\n",
        "  return -(Y - Y_pred) \n",
        "\n",
        "#---------------------------------------------------------------------------LOSS FUNCTION---------------------------------------------------------------------------------------------\n",
        "def cross_entropy():\n",
        "  return\n",
        "\n",
        "def squared_error_loss():\n",
        "  return\n",
        "\n",
        "#----------------------------------------------------------------------INITIALISATION FUNCTIONS---------------------------------------------------------------------------------------\n",
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  np.random.seed(0)\n",
        "  return np.random.randn(*shape)*0.1 #multiplied by 0.1 to have smaller values, to have better initialisation weights\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "    # Calculate the number of inputs and outputs\n",
        "    n_in = shape[0]\n",
        "    n_out = shape[1]    \n",
        "    # Calculate the variance of the normal distribution\n",
        "    variance = 2.0 / (n_in + n_out)\n",
        "    # Initialize the weights with random values drawn from the normal distribution\n",
        "    np.random.seed(0)\n",
        "    weights = np.random.randn(n_in, n_out) * np.sqrt(variance)\n",
        "    \n",
        "    return weights\n",
        "\n",
        "#------------------------------------------------------------------INITIALIZING WEIGHTS AND BIASES------------------------------------------------------------------------------------\n",
        "def initialize_w_b(input_layer,hidden_layer,output_layer,init):\n",
        "  weights=[]\n",
        "  biases=[]\n",
        "  layers=[input_layer]+hidden_layer+[output_layer]\n",
        "  for i in range(len(hidden_layer)+1):\n",
        "     if init=='random':\n",
        "       weights.append(random_initialisation((layers[i+1],layers[i])))\n",
        "     if init=='xavier':\n",
        "       weights.append(xavier_initialisation((layers[i+1],layers[i])))\n",
        "     biases.append(np.random.randn(layers[i+1],1))\n",
        "  return weights, biases\n",
        "\n",
        "'''\n",
        "#testing weights and biases output(used for debugging shape size of matrix) \n",
        "output_layer = 10\n",
        "input_layer = X_test.shape[1]\n",
        "print(X_test.shape)\n",
        "hidden_layer = [64, 64]\n",
        "weights, biases = initialize_w_b(input_layer, hidden_layer, output_layer, 'xavier')\n",
        "for i in range(len(weights)):\n",
        "    print(weights[i].shape)\n",
        "    \n",
        "for i in range(len(biases)):\n",
        "    print(biases[i].shape)\n",
        "\n",
        "weights2, biases2 = initialize_w_b(input_layer, hidden_layer, output_layer, 'random')\n",
        "for i in range(len(weights2)):\n",
        "    print(weights2[i].shape)\n",
        "    \n",
        "for i in range(len(biases2)):\n",
        "    print(biases2[i].shape)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "SIulf2iI377n",
        "outputId": "9bc3d44c-9cc1-44ca-98b2-4f22c5319a0e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#testing weights and biases output(used for debugging shape size of matrix) \\noutput_layer = 10\\ninput_layer = X_test.shape[1]\\nprint(X_test.shape)\\nhidden_layer = [64, 64]\\nweights, biases = initialize_w_b(input_layer, hidden_layer, output_layer, 'xavier')\\nfor i in range(len(weights)):\\n    print(weights[i].shape)\\n    \\nfor i in range(len(biases)):\\n    print(biases[i].shape)\\n\\nweights2, biases2 = initialize_w_b(input_layer, hidden_layer, output_layer, 'random')\\nfor i in range(len(weights2)):\\n    print(weights2[i].shape)\\n    \\nfor i in range(len(biases2)):\\n    print(biases2[i].shape)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network**\n",
        "1. With forward propagation\n",
        "2. Backward propagation with different optimizers\n",
        "3. A train function, which fits and trains the given set"
      ],
      "metadata": {
        "id": "L20aDODvd-I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet():\n",
        "\n",
        "  def __init__(self,input_layer,hidden_layer,output_layer,initialisation_func,act_function,loss_func):\n",
        "     self.input_layer=input_layer\n",
        "     self.hidden_layer=hidden_layer\n",
        "     self.output_layer=output_layer\n",
        "     self.initialisation_func=initialisation_func\n",
        "     self.act_function=act_function\n",
        "     self.loss_func=loss_func\n",
        "     self.weights,self.biases = initialize_w_b(self.input_layer,self.hidden_layer,self.output_layer,self.initialisation_func)\n",
        "     self.layer_size=len(self.hidden_layer)\n",
        "\n",
        "  def forward_propagation(self,X):\n",
        "     #pre-activation\n",
        "     self.a=[]\n",
        "     #post-activation\n",
        "     self.h=[]\n",
        "     l=0\n",
        "     \n",
        "     #pre-activation and post-activation for input layer and first hidden layer\n",
        "     self.a.append((self.weights[l]@X)+self.biases[l])#WX+b\n",
        "     if self.act_function=='sigmoid':\n",
        "       self.h.append(sigmoid(self.a[l]))\n",
        "     elif self.act_function=='relu':\n",
        "       self.h.append(relu(self.a[l]))\n",
        "     elif self.act_function=='tanh':\n",
        "       self.h.append(tanh(self.a[l]))\n",
        "     \n",
        "     #pre-activation and post-activation between hidden layers\n",
        "     for l in range(1,self.layer_size):\n",
        "       self.a.append((self.weights[l]@self.h[l-1])+self.biases[l]) #W.X+b\n",
        "       if self.act_function=='sigmoid':\n",
        "        self.h.append(sigmoid(self.a[l]))\n",
        "       elif self.act_function=='relu':\n",
        "        self.h.append(relu(self.a[l]))\n",
        "       elif self.act_function=='tanh':\n",
        "        self.h.append(tanh(self.a[l]))\n",
        "       \n",
        "     #pre-activation and post-activation between last hidden layer and output layer\n",
        "     l=self.layer_size \n",
        "     self.a.append((self.weights[l]@self.h[l-1])+self.biases[l]) #WX+b\n",
        "     self.h.append(softmax(self.a[l]))\n",
        "     \n",
        "     #we only need the output of the end layer\n",
        "     return self.h[-1]\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------Q3 BACK PROPPAGATION FRAMEWORK WITH OPTIMIZATION FUNCTIONS-----------------------------------------------------------------\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
        "  def back_propagation(self,X,Y):\n",
        "\n",
        "        g_a  = [0]*(self.layer_size+1)\n",
        "        g_h  = [0]*(self.layer_size+1)\n",
        "        g_w  = [0]*(len(self.weights))\n",
        "        g_b  = [0]*(len(self.biases))\n",
        "        #Due to error accumulation the gradients values increases, so we average the value with batch size \n",
        "        batch_size = X.shape[1] \n",
        "\n",
        "        for k in reversed(range(self.layer_size+1)):\n",
        "            #calculating loss function for weights and biases at output\n",
        "            if k == self.layer_size:\n",
        "              if self.loss_func == 'cross_entropy':\n",
        "                  g_a[k] = self.h[k]  - Y  \n",
        "              elif self.loss_func == 'square_loss': \n",
        "                  g_a[k] = (self.h[k] - Y) * self.h[k] * (1 - self.h[k]) \n",
        "            \n",
        "            #calculating gradients for hidden layers     \n",
        "            else:\n",
        "                g_h[k] = (1/batch_size)*((self.weights[k+1].T)@(g_a[k+1]))\n",
        "                if self.act_function == 'sigmoid':\n",
        "                  g_a[k] = (1/batch_size)*((g_h[k])*(grad_sigmoid(self.a[k])))#Here we use '*' operator for elementwise matrix multiplication\n",
        "                elif self.act_function == 'tanh':\n",
        "                  g_a[k] = (1/batch_size)*((g_h[k])*(grad_tanh(self.a[k])))#Here we use '*' operator for elementwise matrix multiplication\n",
        "                elif self.act_function == 'relu':\n",
        "                  g_a[k] = (1/batch_size)*((g_h[k])*(grad_relu(self.a[k])))#Here we use '*' operator for elementwise matrix multiplication\n",
        "                \n",
        "            #calculating gradients of weights \n",
        "            if k == 0:\n",
        "                g_w[k] = (1/batch_size)*((g_a[k])@(X.T)) \n",
        "            else:\n",
        "                g_w[k] = (1/batch_size)*((g_a[k])@(self.h[k-1].T))\n",
        "            \n",
        "            #calculating gradients of biases\n",
        "            g_b[k]  = (1/batch_size)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_w,g_b\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#-----------------------------------------------------STOCHASTIC GRADIENT DESCENT AND OTHER OPTIMIZERS-------------------------------------------------------------------------------\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
        "  def stochastic_gradient_descent(self,W,b,g_w,g_b,lr,lambd):\n",
        "      #Weights=Weights-(learning rate)*(grad weights)-(learning rate*lambda)*(Weights)(this is weight decay with L2 regularization)\n",
        "      W=W-np.multiply(lr,g_w)-np.multiply(lr*lambd,W)\n",
        "\n",
        "      #biases=biases-(learning rate)*(grad biases)\n",
        "      b=b-np.multiply(lr,g_b)\n",
        "      return W,b\n",
        "  \n",
        "  def momentum_based_gradient_descent(self,W,b,g_w,g_b, u_w_i, u_b_i, lr, lambd, gamma):\n",
        "      #u_t=(beta)*(u_t-1)+(grad weight)[u_0 is initialised as zero]\n",
        "      u_w=np.multiply(gamma,u_w_i)+g_w\n",
        "      #weights=weights-(lr)*(updated_weights)-(weight decay L2 regularization)\n",
        "      W=W-np.multiply(lr,u_w)-np.multiply(lr*lambd,W)\n",
        "      \n",
        "      #similarly for biases but without weight decay term\n",
        "      u_b=np.multiply(gamma,u_b_i)+g_b\n",
        "      b=b-np.multiply(lr,u_b)\n",
        "      return W,b,u_w,u_b\n",
        "  \n",
        "  def nesterov_accelerated_gradient_descent(self,W,b,g_w,g_b,lr,lambd,gamma,u_w_i, u_b_i,b_input,b_output):\n",
        "      w_t=W\n",
        "      b_t=b\n",
        "      #here we make changes to global variables as we need to update the weights(look ahead) for calculating gradients\n",
        "      self.weights = self.weights - np.multiply(gamma,u_w_i)\n",
        "      self.biases = self.biases - np.multiply(gamma,u_b_i)\n",
        "      output =  self.forward_propagation(b_input)\n",
        "      #in this step, gradients are recalculated in global variables with updated weight values \n",
        "      g_weights,g_biases = self.back_propagation(b_input,b_output)\n",
        "\n",
        "      #u_t=(gamma)*(u_(t-1))+gradient of(w_t-(gamma)*(u_(t-1)))\n",
        "      u_w=np.multiply(gamma,u_w_i)+g_weights\n",
        "      #weights=weights-(lr)*(updated_weights)-(weight decay L2 regularization)\n",
        "      w_t = w_t - u_w - np.multiply(lr*lambd,w_t)\n",
        "\n",
        "      #similarly for biases but without weight decay term\n",
        "      u_b=np.multiply(gamma,u_b_i)+g_b\n",
        "      b=b-np.multiply(lr,u_b) \n",
        "\n",
        "      return w_t,b,u_w,u_b\n",
        "  \n",
        "  def rmsprop(self,W,b,g_w,g_b,lr,lambd,beta,eps,vw,vb):\n",
        "      #V_t= (beta)*(V_t-1)+(1-beta)*(grad weight)^2 \n",
        "      vw = np.multiply(vw,beta) + np.multiply(1-beta,np.power(g_w,2))           \n",
        "      #similarly for biases\n",
        "      vb = np.multiply(vb,beta) + np.multiply(1-beta,np.power(g_b,2))\n",
        "      \n",
        "      #w_t= (w_t-1)-((lr)/(V_t+eps)^(1/2))*(grad weight)-(L2 regularization weight decay)\n",
        "      W = W - np.multiply(g_w,lr/np.power(vw+eps,1/2))- np.multiply(lr*lambd,W)\n",
        "      #similarly for biases\n",
        "      b = b - np.multiply(g_b,lr/np.power(vb+eps,1/2))\n",
        "      return W,b,vw,vb\n",
        "  \n",
        "  def adam(self,W,b,g_w,g_b,beta1,beta2,lr ,m_t_i ,v_t_i ,m_b_i ,v_b_i,eps,i,lambd):\n",
        "      \n",
        "      m_t = np.multiply(beta1,m_t_i) + np.multiply(1-beta1,g_w)\n",
        "      v_t = np.multiply(beta2,v_t_i) + np.multiply(1-beta2,np.power(g_w,2))\n",
        "      m_b = np.multiply(beta1,m_b_i) + np.multiply(1-beta1,g_b)\n",
        "      v_b = np.multiply(beta2,v_b_i) + np.multiply(1-beta2,np.power(g_b,2))\n",
        "                \n",
        "      #normalization of moment          \n",
        "      m_hat_w = m_t/(1 - np.power(beta1,i+1))\n",
        "      m_hat_b = m_b/(1 - np.power(beta1,i+1))\n",
        "\n",
        "      #normalization          \n",
        "      v_hat_w = v_t/(1 - np.power(beta2,i+1))\n",
        "      v_hat_b = v_b/(1 - np.power(beta2,i+1))\n",
        "      \n",
        "      W = W - ((lr / np.power(v_hat_w + eps, 1/2)) * m_hat_w) - np.multiply(lr*lambd,W)\n",
        "      \n",
        "      b = b - ((lr / np.power(v_hat_b + eps, 1/2)) * m_hat_b)\n",
        "      return W,b,m_t,v_t,m_b,v_b\n",
        "  \n",
        "  def nadam(self,W,b,g_w,g_b,beta1,beta2,lr ,m_t_i ,v_t_i ,m_b_i ,v_b_i,eps,i,lambd):\n",
        "      m_t =  np.multiply(beta1,m_t_i) + np.multiply(1 - beta1,g_w)\n",
        "      v_t =  np.multiply(beta2,v_t_i) + np.multiply(1 - beta2,np.power(g_w, 2))\n",
        "\n",
        "      m_b =  np.multiply(beta1,m_b_i) + np.multiply(1 - beta1,g_b)\n",
        "      v_b =  np.multiply(beta2,v_b_i) + np.multiply(1 - beta2,np.power(g_b, 2))\n",
        "                \n",
        "      m_hat_w = m_t / (1 - np.power(beta1, i+1)) \n",
        "      v_hat_t = v_t / (1 - np.power(beta2, i+1))\n",
        "\n",
        "      m_hat_b = m_b / (1 - np.power(beta1, i+1)) \n",
        "      v_hat_b = v_b / (1 - np.power(beta2, i+1))\n",
        "  \n",
        "      a1 = (1-beta1)/(1-np.power(beta1,i+1))\n",
        "      update_w = np.multiply(lr/(np.power(v_hat_t + eps,1/2)),(np.multiply(a1,g_w) + np.multiply(beta1,m_hat_w)))\n",
        "      update_b = np.multiply(lr/(np.power(v_hat_b + eps,1/2)),(np.multiply(a1,g_b)+np.multiply(beta1,m_hat_b) ))\n",
        "      W = W - update_w - np.multiply(lr*lambd,W)\n",
        "      b = b - update_b    \n",
        "      return W,b,m_t,v_t,m_b,v_b\n",
        "  \n",
        "\n",
        "\n",
        "  def predict(self, X,y ):\n",
        "      output =  self.forward_propagation(X)\n",
        "      out_class=(np.argmax(output,axis=0))\n",
        "      accuracy = round(accuracy_score(y, out_class) * 100, 3)\n",
        "      return accuracy , out_class\n",
        "\n",
        "  def train(self,X_train,y_train,X_val ,y_val ,learning_rate,epochs, optimiser='gd',batch_size = 64,lambd=0.0005): \n",
        "\n",
        "      update_w = np.zeros(np.array(self.weights).shape)\n",
        "      update_b = np.zeros(np.array(self.biases).shape)\n",
        "      update_w_i = np.zeros(np.array(self.weights).shape)\n",
        "      update_b_i = np.zeros(np.array(self.biases).shape)\n",
        "      vw_i = 0.0\n",
        "      vb_i = 0.0\n",
        "      m_t_i=0\n",
        "      v_t_i=0\n",
        "      m_b_i=0\n",
        "      v_b_i=0\n",
        "      eps = 1e-8\n",
        "      gamma = 0.9\n",
        "      beta = 0.999\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      m_t, v_t, m_hat_w, v_hat_w, m_b,v_b,m_hat_b,v_hat_b = 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 \n",
        "      train_accuracy = []\n",
        "      val_accuracy = []\n",
        "      training_loss ,validation_loss = [] ,[]\n",
        "      for i in tqdm(range(int(epochs))):\n",
        "        print(i)\n",
        "        for batch in range(0, X_train.shape[1], batch_size):\n",
        "\n",
        "          batch_images =  X_train[:,batch:batch+batch_size]\n",
        "          batch_output =  Y_train_encoded[:,batch:batch+batch_size]\n",
        "          output =  self.forward_propagation(batch_images)\n",
        "          g_weights,g_biases = self.back_propagation(batch_images,batch_output)\n",
        "          if optimiser == 'gd':\n",
        "              self.weights,self.biases=self.stochastic_gradient_descent(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd)\n",
        "          \n",
        "          if optimiser == 'mgd':\n",
        "              self.weights,self.biases,update_w,update_b=self.momentum_based_gradient_descent(self.weights,self.biases,g_weights,g_biases,update_w_i,update_b_i,learning_rate,lambd,gamma)\n",
        "              update_w_i = update_w\n",
        "              update_b_i = update_b\n",
        "\n",
        "          if optimiser == 'ngd':\n",
        "              w_t,b_t,update_w,update_b=self.nesterov_accelerated_gradient_descent(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd,gamma,update_w_i,update_b_i,batch_images,batch_output)\n",
        "              self.weights = w_t\n",
        "              self.biases = b_t\n",
        "              update_w_i = update_w\n",
        "              update_b_i = update_b\n",
        "\n",
        "\n",
        "          if optimiser == 'rmsprop': \n",
        "              self.weights,self.biases,vw,vb= self.rmsprop(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd,beta,eps,vw_i,vb_i)\n",
        "              vw_i=vw\n",
        "              vb_i=vb\n",
        "\n",
        "          if optimiser == 'adam':\n",
        "              self.weights,self.biases,m_t,v_t,m_b,v_b=self.adam(self.weights,self.biases,g_weights,g_biases,beta1,beta2,learning_rate, m_t_i, v_t_i, m_b_i, v_b_i,eps,i,lambd)\n",
        "              m_t_i=m_t\n",
        "              v_t_i=v_t\n",
        "              m_b_i=m_b\n",
        "              v_b_i=v_b\n",
        "\n",
        "          \n",
        "          if optimiser == 'nadam':\n",
        "              self.weights,self.biases,m_t,v_t,m_b,v_b=self.nadam(self.weights,self.biases,g_weights,g_biases,beta1,beta2,learning_rate, m_t_i,v_t_i, m_b_i,v_b_i, eps, i, lambd)\n",
        "              m_t_i=m_t\n",
        "              v_t_i=v_t\n",
        "              m_b_i=m_b\n",
        "              v_b_i=v_b                         \n",
        "\n",
        "\n",
        "        #Training loss for full dataset\n",
        "        predicted_train = self.forward_propagation(X_train)\n",
        "        predicted_train_label=(np.argmax(predicted_train,axis=0)) \n",
        "        acc1 = 100*np.sum(predicted_train_label==y_train)/predicted_train.shape[1]\n",
        "        train_accuracy.append(acc1)\n",
        "\n",
        "\n",
        "        predicted_val = self.forward_propagation(X_val)\n",
        "        predicted_val_label=(np.argmax(predicted_val,axis=0))\n",
        "        acc2 = 100*np.sum(predicted_val_label==y_val)/predicted_val.shape[1]   \n",
        "        val_accuracy.append(acc2)\n",
        "\n",
        "        a =self.weights[1:len(self.hidden_layer)]\n",
        "        b = np.sum([(np.sum((a[i]**2).reshape(1,-1))) for i in range(len(a))]) \n",
        "        if self.loss_func == 'cross_entropy':\n",
        "          train_loss = (-np.sum(np.multiply(Y_train_encoded,np.log(predicted_train)))+((lambd/2.)*b))/y_train.shape[0]\n",
        "          val_loss = (-np.sum(np.multiply(Y_val_encoded,np.log(predicted_val)))+((lambd/2.)*b))/y_val.shape[0]\n",
        "\n",
        "        elif self.loss_func == 'square_loss':\n",
        "\n",
        "          val_loss =((1/2) * np.sum((Y_val_encoded- predicted_val)**2))/y_val.shape[0]\n",
        "          train_loss = (1/2) * np.sum((Y_train_encoded - predicted_train)**2)/y_train.shape[0]\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(val_loss)\n",
        "          \n",
        "\n",
        "        print('Epoch {} : training_accuracy = {:.2f}, training_loss = {:.5f},Validation accuracy = {:.2f},Validation loss = {:.5f}'.format(i+1,acc1,train_loss, acc2,val_loss))\n",
        "        wandb.log({\"val_accuracy\": acc2,\"accuracy\": acc1,\"steps\":epochs,\"train_loss\":train_loss,\"val_loss\":val_loss},)\n",
        "        \n",
        "      \n",
        "      return train_accuracy,val_accuracy,training_loss, validation_loss\n",
        "\n",
        "# sweep_config = {\n",
        "#     'method': 'grid', #grid, random,bayes\n",
        "#     'metric': {\n",
        "#       'name': 'val_accuracy',\n",
        "#       'goal': 'maximize'  \n",
        "#     },\n",
        "#     'parameters': {\n",
        "#         'epochs': {\n",
        "#             'values': [5,10,15,20]\n",
        "#         },\n",
        "#         'learning_rate': {\n",
        "#             'values': [0.001,0.0001]\n",
        "#         },\n",
        "#         'loss_function':{\n",
        "#             'values':['cross_entropy']\n",
        "#         },\n",
        "#         'initilisation':{\n",
        "#             'values':['xavier']\n",
        "#         },\n",
        "#         'batch_size':{\n",
        "#             'values':[32,64,128]\n",
        "#         },\n",
        "#         'optimiser': {\n",
        "#             'values': ['gd','mgd','ngd', 'rmsprop','adam','nadam']\n",
        "#         },\n",
        "#         'activation': {\n",
        "#             'values': ['relu','tanh','sigmoid']\n",
        "#         },\n",
        "#         'hidden_layer': {\n",
        "#             'values': [[32,64,128],\n",
        "#                        [32,32,32],[32,32,32,32],[32,32,32,32,32],[64,64,64],\n",
        "#                        [64,64,64,64],[64,64,64,64,64],[128,128,128],\n",
        "#                        [128,128,128,128],[128,128,128,128,128]]\n",
        "#         },\n",
        "        \n",
        "#         'weight_decay':{\n",
        "#             'values':[0.0,0.0005]\n",
        "#         }\n",
        "        \n",
        "#     }\n",
        "# }\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, entity='shreyashgadgil007', project=\"CS-6910 A1\")\n",
        "\n",
        "def train1():\n",
        "  print('me')\n",
        "  steps = 10\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults = {\n",
        "      'epochs': 5,\n",
        "      'input_layer': 784,\n",
        "      'output_layer': 10,\n",
        "      'batch_size':64,\n",
        "      'weight_decay':0.005,\n",
        "      'learning_rate': 0.0001,\n",
        "      'hidden_layer':[16,16,16],\n",
        "      'optimiser':'mgd',\n",
        "      'activation':'sigmoid',\n",
        "      'initialisation':'xavier',\n",
        "      'loss_function':'cross_entropy'\n",
        "      \n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  wandb.init(project='CS-6910 A1', entity='shreyashgadgil007',config=config_defaults)\n",
        "  \n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "  learning_rate = config.learning_rate\n",
        "  epochs = int(config.epochs)\n",
        "  hidden_layer = config.hidden_layer\n",
        "  activation = config.activation\n",
        "  optimiser = config.optimiser\n",
        "  input_layer = config.input_layer\n",
        "  output_layer = config.output_layer\n",
        "  batch_size = config.batch_size\n",
        "  weight_decay = config.weight_decay\n",
        "  loss_function = config.loss_function\n",
        "  initialisation = config.initilisation\n",
        "  # Model training here\n",
        "  sweep_network = NeuralNet(784, [32,32,32], 10 ,'xavier','sigmoid',\"cross_entropy\")\n",
        "  sweep_network.train(X_train,Y_train,X_val,Y_val,0.001,10,'gd',32,0.005)\n",
        "  # print('Acc',acc2)\n",
        "\n",
        "#wandb.agent(sweep_id, function=train1, count=100)\n",
        "\n",
        "train1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "4QHPBlX6Q5gF",
        "outputId": "78b27775-6082-4f23-aeb7-f136078896e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "me\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshreyashgadgil007\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230309_032036-0znlljlo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shreyashgadgil007/CS-6910%20A1/runs/0znlljlo' target=\"_blank\">jolly-wood-72</a></strong> to <a href='https://wandb.ai/shreyashgadgil007/CS-6910%20A1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/shreyashgadgil007/CS-6910%20A1' target=\"_blank\">https://wandb.ai/shreyashgadgil007/CS-6910%20A1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/shreyashgadgil007/CS-6910%20A1/runs/0znlljlo' target=\"_blank\">https://wandb.ai/shreyashgadgil007/CS-6910%20A1/runs/0znlljlo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_config.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mke\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_config.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'initilisation'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-42f4af825b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;31m#wandb.agent(sweep_id, function=train1, count=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m \u001b[0mtrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-42f4af825b19>\u001b[0m in \u001b[0;36mtrain1\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m   \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m   \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m   \u001b[0minitialisation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitilisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m   \u001b[0;31m# Model training here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m   \u001b[0msweep_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'xavier'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cross_entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_config.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mke\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;34mf\"{self.__class__!r} object has no attribute {key!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             ) from ke\n",
            "\u001b[0;31mAttributeError\u001b[0m: <class 'wandb.sdk.wandb_config.Config'> object has no attribute 'initilisation'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 Forward propagation solved"
      ],
      "metadata": {
        "id": "uBo3daWQqabz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_class = 10\n",
        "\n",
        "model = NeuralNet(28*28,[32,32,32],n_class, initialisation_func = 'random', act_function='tanh', loss_func = 'cross_entropy')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "acc1,acc2,train_loss, val_loss= model.train(X_train,Y_train,X_val ,Y_val ,learning_rate = 0.01,epochs=10, optimiser='adam',batch_size =32,lambd=0.0005)\n"
      ],
      "metadata": {
        "id": "fVHUzQYN7TOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1    = NeuralNet(X_train.shape[0],[64,64,64], 10,'xavier','sigmoid','cross_entropy')\n",
        "A=q1.forward_propagation(X_train)\n",
        "A=A.T\n",
        "print(A[0])\n",
        "print(X_train.shape[1])\n",
        "print(Y_train[0])\n",
        "print((Y_train_encoded.T)[0])"
      ],
      "metadata": {
        "id": "RPh4aKLwnXfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "B=np.array([[2,3],[3,4]])\n",
        "A=np.zeros(B.shape)\n",
        "print(A)"
      ],
      "metadata": {
        "id": "Nd5r_G8TX-Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid', #grid, random,bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'  \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10,15,20]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-2,1e-3]\n",
        "        },\n",
        "        'loss_function':{\n",
        "            'values':['cross_entropy']\n",
        "        },\n",
        "        'initilisation':{\n",
        "            'values':['xavier']\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[32,64,128]\n",
        "        },\n",
        "        'optimiser': {\n",
        "            'values': ['gd','mgd','ngd', 'rmsprop','adam','nadam']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu','tanh','sigmoid']\n",
        "        },\n",
        "        'hidden_layer': {\n",
        "            'values': [[32,64,128],\n",
        "                       [32,32,32],[32,32,32,32],[32,32,32,32,32],[64,64,64],\n",
        "                       [64,64,64,64],[64,64,64,64,64],[128,128,128],\n",
        "                       [128,128,128,128],[128,128,128,128,128]]\n",
        "        },\n",
        "        \n",
        "        'weight_decay':{\n",
        "            'values':[0.0,0.0005]\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "CmblYgPneasR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Fj2Hcd953Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade wandb\n",
        "!wandb login aa5afea12b4fda1e7f8310b597eb17c73d1176d2 #my API key for wandb login \n",
        "sweep_id = wandb.sweep(sweep_config, entity='shreyashgadgil007', project=\"CS-6910 A1\")"
      ],
      "metadata": {
        "id": "Eyvfhw2OecHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train1():\n",
        "    steps = 10\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 5,\n",
        "        'input_layer': 784,\n",
        "        'output_layer': 10,\n",
        "        'batch_size':64,\n",
        "        'weight_decay':0.005,\n",
        "        'learning_rate': 1e-3,\n",
        "        'hidden_layer':[16,16,16],\n",
        "        'learning_rate':1e-2,\n",
        "        'optimiser':'mgd',\n",
        "        'activation':'sigmoid',\n",
        "        'initialisation':'xavier',\n",
        "        'loss_function':'cross_entropy'\n",
        "        \n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(project='CS-6910 A1', entity='shreyashgadgil007',config=config_defaults)\n",
        "    \n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    learning_rate = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    hidden_layer = config.hidden_layer\n",
        "    activation = config.activation\n",
        "    optimiser = config.optimiser\n",
        "    input_layer = config.input_layer\n",
        "    output_layer = config.output_layer\n",
        "    batch_size = config.batch_size\n",
        "    weight_decay = config.weight_decay\n",
        "    loss_function = config.loss_function\n",
        "    initialisation = config.initilisation\n",
        "    # Model training here\n",
        "    sweep_network    = NeuralNet(input_layer, hidden_layer, output_layer,initialisation,activation,loss_function)\n",
        "    acc1,acc2,train_loss,val_loss  = sweep_network.train(X_train,Y_train,X_val,Y_val,epochs,learning_rate,optimiser,batch_size,weight_decay)"
      ],
      "metadata": {
        "id": "FARZ7Bfbirzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "wandb.agent(sweep_id, function=train1, count=100)"
      ],
      "metadata": {
        "id": "1svx8hu2jcbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
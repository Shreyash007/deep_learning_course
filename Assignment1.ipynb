{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP8GOW4ReY9avObu6cOdsl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyash007/deep_learning_course/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 1 CS6910**\n",
        "Shreyash Gadgil (ED22S016)"
      ],
      "metadata": {
        "id": "9LINEVNo0Fo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login aa5afea12b4fda1e7f8310b597eb17c73d1176d2 #my API key for wandb login "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC2z2tS8Ccxo",
        "outputId": "df8d219f-7c1c-4a5e-f760-0c3fc022fcd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=a3b6eedd981c389545a1a2f72bef2e2c52f0f5a4c7bad7e57a83ba0dd0a6d09b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, GitPython, wandb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 urllib3-1.26.14 wandb-0.13.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "G4iCjM7Q0SiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P0c3goK0vrQL"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1 Solution**\n",
        "Labelling dataset"
      ],
      "metadata": {
        "id": "a0Q5Mjvy0XZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "#importing dataset\n",
        "\n",
        "#checking size of training and test dataset\n",
        "print(\"Training dataset shape, X=\",X_train.shape,\", Y=\",Y_train.shape)\n",
        "print(\"Test dataset shape, X=\",X_test.shape,\", Y=\",Y_test.shape)\n",
        "\n",
        "n_img_classes=len(np.unique(Y_train)) #to find out number of unique images \n",
        "img_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] #taken from keras documentation\n",
        "\n",
        "\n",
        "def show_images(n_img_classes,img_labels,X,Y):\n",
        "\n",
        "   setImgs = []\n",
        "   setLabels=[]\n",
        "\n",
        "   for i in range(len(X)):\n",
        "     if len(setImgs)==n_img_classes:\n",
        "       break\n",
        "     if img_labels[Y[i]] not in setLabels:\n",
        "      setImgs.append(X[i])\n",
        "      setLabels.append(img_labels[Y[i]])\n",
        "\n",
        "   run=wandb.init(project='CS-6910 A1',entity='shreyashgadgil007',reinit=True)\n",
        "   run.log({\"Sample from each class\":list(wandb.Image(img,caption= caption) for img,caption in zip(setImgs,setLabels))})\n",
        "   run.finish()\n",
        "\n",
        "#--------------------------------------------------------------------------Q1 SOLUTION-----------------------------------------------------------------------------------------------\n",
        "#show_images(n_img_classes,img_labels,X_test,Y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPtktkP0cLG",
        "outputId": "549c797c-526a-4c00-f5fc-79bd7d9a5476"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training dataset shape, X= (60000, 28, 28) , Y= (60000,)\n",
            "Test dataset shape, X= (10000, 28, 28) , Y= (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing dataset for operations**\n",
        "1. Creating training,test,validation split for dataset\n",
        "2. Reshaping image matrix and normalizing pixel values\n",
        "3. One hot encoding classifications\n"
      ],
      "metadata": {
        "id": "ySDC6Ck0GjBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess():\n",
        "  #change the code below to accept different dataset\n",
        "  (X_1, Y_1), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "  \n",
        "  #importing dataset again and normalizing\n",
        "  X_1 = X_1.reshape(X_1.shape[0],-1)/255.0\n",
        "  X_test = X_test.reshape(X_test.shape[0],-1)/255.0\n",
        "\n",
        "  #training and validation split as specified in the question 10%\n",
        "  X_train, X_val, Y_train, Y_val= train_test_split(X_1,Y_1,test_size=0.1,random_state=0)\n",
        "  \n",
        "  #one hot encoding\n",
        "  Y_train_encoded=np.zeros((Y_train.shape[0],10))\n",
        "  for i in range(len(Y_train)):\n",
        "    Y_train_encoded[i][Y_train[i]]=1\n",
        "\n",
        "  Y_val_encoded=np.zeros((Y_val.shape[0],10))\n",
        "  for i in range(len(Y_val)):\n",
        "    Y_val_encoded[i][Y_val[i]]=1\n",
        "\n",
        "  Y_test_encoded=np.zeros((Y_test.shape[0],10))\n",
        "  for i in range(len(Y_test)):\n",
        "    Y_test_encoded[i][Y_test[i]]=1\n",
        "\n",
        "  return X_train,X_test,X_val,Y_train,Y_val,Y_test,Y_train_encoded,Y_val_encoded,Y_test_encoded\n",
        "\n",
        "#Below variables will be used in the entire code\n",
        "X_train,X_test,X_val,Y_train,Y_val,Y_test,Y_train_encoded,Y_val_encoded,Y_test_encoded=dataset_preprocess()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuMSSFIfGiZ_",
        "outputId": "46ae7dc5-1886-4910-e9aa-ee23f863f6a0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper functions**\n",
        "1. Declared activation functions\n",
        "2. Declared initialisation functions\n",
        "3. Declared a function to initialize weights and biases"
      ],
      "metadata": {
        "id": "XCmVKBiE362R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------ACTIVATION FUNCTIONS AND THEIR GRADIENTS-------------------------------------------------------------------------------\n",
        "def relu(X):\n",
        "  return max(X,0)\n",
        "\n",
        "def grad_relu(X):\n",
        "  return X>0\n",
        "\n",
        "def sigmoid(X):\n",
        "  return 1/(1+np.exp(-X))\n",
        "\n",
        "def grad_sigmoid(X):\n",
        "  return sigmoid(X)(1-sigmoid(X))\n",
        "\n",
        "def tanh(X):\n",
        "  return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "\n",
        "def grad_tanh(X):\n",
        "  return 1-(tanh(X))**2\n",
        "\n",
        "def softmax(X):\n",
        "  # Calculates the softmax function\n",
        "  e_X = np.exp(X - np.max(X, axis = 0))\n",
        "  return e_X / e_X.sum(axis = 0)\n",
        "\n",
        "def softmax_cross_entropy(Y,Y_pred):\n",
        "  return -(Y - Y_pred) \n",
        "\n",
        "#---------------------------------------------------------------------------LOSS FUNCTION---------------------------------------------------------------------------------------------\n",
        "def cross_entropy():\n",
        "  return\n",
        "\n",
        "def squared_error_loss():\n",
        "  return\n",
        "\n",
        "#----------------------------------------------------------------------INITIALISATION FUNCTIONS---------------------------------------------------------------------------------------\n",
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  np.random.seed(0)\n",
        "  return np.random.randn(*shape)*0.1\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "    # Calculate the number of inputs and outputs\n",
        "    n_in = shape[0]\n",
        "    n_out = shape[1]\n",
        "    \n",
        "    # Calculate the variance of the normal distribution\n",
        "    variance = 2.0 / (n_in + n_out)\n",
        "    \n",
        "    # Initialize the weights with random values drawn from the normal distribution\n",
        "    np.random.seed(0)\n",
        "    weights = np.random.randn(n_in, n_out) * np.sqrt(variance)\n",
        "    \n",
        "    return weights\n",
        "\n",
        "#------------------------------------------------------------------INITIALIZING WEIGHTS AND BIASES------------------------------------------------------------------------------------\n",
        "def initialize_w_b(input_layer,hidden_layer,output_layer,init):\n",
        "  weights=[]\n",
        "  biases=[]\n",
        "  layers=[input_layer]+hidden_layer+[output_layer]\n",
        "  for i in range(len(hidden_layer)+1):\n",
        "     if init=='random':\n",
        "       weights.append(random_initialisation((layers[i+1],layers[i])))\n",
        "     if init=='xavier':\n",
        "       weights.append(xavier_initialisation((layers[i+1],layers[i])))\n",
        "     biases.append(np.random.randn(layers[i+1],1))\n",
        "  return weights, biases\n",
        "\n",
        "#testing weights and biases output(used for debugging shape size of matrix) \n",
        "output_layer = 10\n",
        "input_layer = X_test.shape[1]\n",
        "print(X_test.shape)\n",
        "hidden_layer = [64, 64]\n",
        "weights, biases = initialize_w_b(input_layer, hidden_layer, output_layer, 'xavier')\n",
        "for i in range(len(weights)):\n",
        "    print(weights[i].shape)\n",
        "    \n",
        "for i in range(len(biases)):\n",
        "    print(biases[i].shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIulf2iI377n",
        "outputId": "0fb29823-ce05-46ee-d811-c34eb3725f43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 784)\n",
            "(64, 784)\n",
            "(64, 64)\n",
            "(10, 64)\n",
            "(64, 1)\n",
            "(64, 1)\n",
            "(10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network**\n",
        "1. With forward propagation\n",
        "2. Backward propagation\n",
        "3. Weight updates w.r.t different functions"
      ],
      "metadata": {
        "id": "L20aDODvd-I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet():\n",
        "\n",
        "  def __init__(self,input_layer,hidden_layer,output_layer,initialisation_func,act_function,loss_func):\n",
        "     self.input_layer=input_layer\n",
        "     self.hidden_layer=hidden_layer\n",
        "     self.output_layer=output_layer\n",
        "     self.initialisation_func=initialisation_func\n",
        "     self.act_function=act_function\n",
        "     self.loss_func=loss_func\n",
        "     self.weights,self.biases = initialize_w_b(self.input_layer,self.hidden_layer,self.output_layer,self.initialisation_func)\n",
        "\n",
        "  def forward_propagation(self,X):\n",
        "     #pre-activation\n",
        "     self.A=[]\n",
        "     #post-activation\n",
        "     self.H=[]\n",
        "     l=0\n",
        "     \n",
        "     #pre-activation and post-activation for input layer and first hidden layer\n",
        "     self.A.append(np.matmul(self.weights[l],X)+self.biases[l])\n",
        "     if self.act_function=='sigmoid':\n",
        "       self.H.append(sigmoid(self.A[l]))\n",
        "     elif self.act_function=='relu':\n",
        "       self.H.append(relu(self.A[l]))\n",
        "     elif self.act_function=='tanh':\n",
        "       self.H.append(tanh(self.A[l]))\n",
        "     \n",
        "\n",
        "     #pre-activation and post-activation between hidden layers\n",
        "     for l in range(1,len(self.hidden_layer)):\n",
        "       self.A.append(np.matmul(self.weights[l],self.H[l-1])+self.biases[l])\n",
        "       if self.act_function=='sigmoid':\n",
        "        self.H.append(sigmoid(self.A[l]))\n",
        "       elif self.act_function=='relu':\n",
        "        self.H.append(relu(self.A[l]))\n",
        "       elif self.act_function=='tanh':\n",
        "        self.H.append(tanh(self.A[l]))\n",
        "       \n",
        "     #pre-activation and post-activation between last hidden layer and output layer\n",
        "     l=len(self.hidden_layer)\n",
        "     self.A.append(np.matmul(self.weights[l],self.H[l-1])+self.biases[l])\n",
        "     self.H.append(softmax(self.A[l]))\n",
        "\n",
        "     return self.H[-1]\n",
        "  \n",
        "def back_propagation(self,X,Y):\n",
        "        #Initialisng gradient parameters\n",
        "        #Takes input image and its label and returns the gradient parameters i.e. gradient of weight and gradient of biases\n",
        "        g_a  = [0]*(len(self.hidden_layer)+1)\n",
        "        g_h  = [0]*(len(self.hidden_layer)+1)\n",
        "        g_w  = [0]*(len(self.weights))\n",
        "        g_b  = [0]*(len(self.biases))\n",
        "\n",
        "        n_samples = X.shape[1]  # represents batch size\n",
        "\n",
        "\n",
        "        for l in reversed(range(len(self.hidden_layer)+1)):\n",
        "            if l == len(self.hidden_layer):\n",
        "              #change below to functions for calls\n",
        "              if self.loss_function == 'cross_entropy':\n",
        "                  g_a[l] = self.H[l]  - Y  \n",
        "              elif self.loss_function == 'square_loss': \n",
        "                  g_a[l] = (self.H[l] - Y) * self.H[l] * (1 - self.H[l]) \n",
        "                \n",
        "            else:\n",
        "                g_h[l] = (1/n_samples)*np.matmul(self.weights[l+1].T,g_a[l+1])\n",
        "                if self.activation == 'sigmoid':\n",
        "                  g_a[l] = (1/n_samples)*np.multiply(g_h[l],grad_sigmoid(self.A[l]))\n",
        "                elif self.activation == 'tanh':\n",
        "                  g_a[l] = (1/n_samples)*np.multiply(g_h[l],grad_tanh(self.A[l]))\n",
        "                elif self.activation == 'relu':\n",
        "                  g_a[l] = (1/n_samples)*np.multiply(g_h[l],grad_relu(self.A[l]))\n",
        "                \n",
        "\n",
        "            if l == 0:\n",
        "                g_w[l] = (1/n_samples)*np.matmul(g_a[l],X_train.T) \n",
        "            else:\n",
        "                g_w[l] = (1/n_samples)*np.matmul(g_a[l],self.H[l-1].T)\n",
        "\n",
        "            g_b[l]  = (1/n_samples)*np.sum(g_a[l], axis=1, keepdims = True)\n",
        "        return g_w,g_b\n"
      ],
      "metadata": {
        "id": "4QHPBlX6Q5gF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 Forward propagation solved"
      ],
      "metadata": {
        "id": "uBo3daWQqabz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1    = NeuralNet(X_train.shape[1],[64,64,64], 10,'xavier','sigmoid','cross_entropy')\n",
        "A=q1.forward_propagation(X_train.T)\n",
        "A=A.T\n",
        "print(A[0])\n",
        "print((X_train.T).shape[0])\n",
        "print(Y_train[0])\n",
        "print(Y_train_encoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPh4aKLwnXfT",
        "outputId": "45b0bf30-ebbb-4baa-c098-784fc6214592"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05723773 0.55586946 0.05466115 0.02037356 0.00672244 0.00941655\n",
            " 0.1432002  0.06383334 0.01125722 0.07742835]\n",
            "784\n",
            "9\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}
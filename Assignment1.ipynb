{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN9XdVtOZ+5jkIxHm/7KbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyash007/deep_learning_course/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 1 CS6910**\n",
        "Shreyash Gadgil (ED22S016)"
      ],
      "metadata": {
        "id": "9LINEVNo0Fo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login aa5afea12b4fda1e7f8310b597eb17c73d1176d2 #my API key for wandb login "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC2z2tS8Ccxo",
        "outputId": "7c518a64-4b33-4e94-bfdd-0a235fb256ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=ba7aad53d0f34f4d2d92071a97b1d268879e504f14d92f4409b50337003e950c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "G4iCjM7Q0SiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P0c3goK0vrQL"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import wandb\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1 Solution**\n",
        "Labelling dataset"
      ],
      "metadata": {
        "id": "a0Q5Mjvy0XZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "#importing dataset(add this in show_images function as variable)\n",
        "\n",
        "#checking size of training and test dataset\n",
        "print(\"Training dataset shape, X=\",X_train.shape,\", Y=\",Y_train.shape)\n",
        "print(\"Test dataset shape, X=\",X_test.shape,\", Y=\",Y_test.shape)\n",
        "\n",
        "n_img_classes=len(np.unique(Y_train)) #to find out number of unique images \n",
        "img_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] #taken from keras documentation\n",
        "\n",
        "\n",
        "def show_images(n_img_classes,img_labels,X,Y):\n",
        "\n",
        "   setImgs = []\n",
        "   setLabels=[]\n",
        "\n",
        "   for i in range(len(X)):\n",
        "     if len(setImgs)==n_img_classes:\n",
        "       break\n",
        "     if img_labels[Y[i]] not in setLabels:\n",
        "      setImgs.append(X[i])\n",
        "      setLabels.append(img_labels[Y[i]])\n",
        "\n",
        "   run=wandb.init(project='CS-6910 A1',entity='shreyashgadgil007',reinit=True)\n",
        "   run.log({\"Sample from each class\":list(wandb.Image(img,caption= caption) for img,caption in zip(setImgs,setLabels))})\n",
        "   run.finish()\n",
        "\n",
        "#--------------------------------------------------------------------------Q1 SOLUTION-----------------------------------------------------------------------------------------------\n",
        "#show_images(n_img_classes,img_labels,X_test,Y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPtktkP0cLG",
        "outputId": "9dcf7bc3-6d49-4245-a319-c90fdc786849"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training dataset shape, X= (60000, 28, 28) , Y= (60000,)\n",
            "Test dataset shape, X= (10000, 28, 28) , Y= (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing dataset for operations**\n",
        "1. Creating training,test,validation split for dataset\n",
        "2. Reshaping image matrix and normalizing pixel values\n",
        "3. One hot encoding classifications\n"
      ],
      "metadata": {
        "id": "ySDC6Ck0GjBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess():\n",
        "  #change the code below to accept different dataset\n",
        "  (X_1, Y_1), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "  \n",
        "  #importing dataset again and normalizing\n",
        "  X_1 = X_1.reshape(X_1.shape[0],-1)/255.0\n",
        "  X_test = X_test.reshape(X_test.shape[0],-1)/255.0\n",
        "\n",
        "  #training and validation split as specified in the question 10%\n",
        "  X_train, X_val, Y_train, Y_val= train_test_split(X_1,Y_1,test_size=0.1,random_state=0)\n",
        "  \n",
        "  #one hot encoding\n",
        "  Y_train_encoded=np.zeros((Y_train.shape[0],10))\n",
        "  for i in range(len(Y_train)):\n",
        "    Y_train_encoded[i][Y_train[i]]=1\n",
        "\n",
        "  Y_val_encoded=np.zeros((Y_val.shape[0],10))\n",
        "  for i in range(len(Y_val)):\n",
        "    Y_val_encoded[i][Y_val[i]]=1\n",
        "\n",
        "  Y_test_encoded=np.zeros((Y_test.shape[0],10))\n",
        "  for i in range(len(Y_test)):\n",
        "    Y_test_encoded[i][Y_test[i]]=1\n",
        "\n",
        "  return X_train.T,X_test.T,X_val.T,Y_train.T,Y_val.T,Y_test.T,Y_train_encoded.T,Y_val_encoded.T,Y_test_encoded.T\n",
        "\n",
        "#Below variables will be used in the entire code\n",
        "X_train,X_test,X_val,Y_train,Y_val,Y_test,Y_train_encoded,Y_val_encoded,Y_test_encoded=dataset_preprocess()\n",
        "print(Y_val_encoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuMSSFIfGiZ_",
        "outputId": "7cbb7772-5eda-4e45-a74b-b58d49515db6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper functions**\n",
        "1. Declared activation functions\n",
        "2. Declared initialisation functions\n",
        "3. Declared a function to initialize weights and biases"
      ],
      "metadata": {
        "id": "XCmVKBiE362R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------ACTIVATION FUNCTIONS AND THEIR GRADIENTS-------------------------------------------------------------------------------\n",
        "def relu(X):\n",
        "  return np.maximum(0,X)\n",
        "\n",
        "def grad_relu(X):\n",
        "  return X>0\n",
        "\n",
        "def sigmoid(X):\n",
        "  return 1/(1+np.exp(-X))\n",
        "\n",
        "def grad_sigmoid(X):\n",
        "  return (sigmoid(X))*(1-sigmoid(X))\n",
        "\n",
        "def tanh(X):\n",
        "  return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "\n",
        "def grad_tanh(X):\n",
        "  return 1-(tanh(X))**2\n",
        "\n",
        "def softmax(X):\n",
        "  # Calculates the softmax function\n",
        "  e_X = np.exp(X - np.max(X, axis = 0))\n",
        "  return e_X / e_X.sum(axis = 0)\n",
        "\n",
        "def softmax_cross_entropy(Y,Y_pred):\n",
        "  return -(Y - Y_pred) \n",
        "\n",
        "#---------------------------------------------------------------------------LOSS FUNCTION---------------------------------------------------------------------------------------------\n",
        "def cross_entropy():\n",
        "  return\n",
        "\n",
        "def squared_error_loss():\n",
        "  return\n",
        "\n",
        "#----------------------------------------------------------------------INITIALISATION FUNCTIONS---------------------------------------------------------------------------------------\n",
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  np.random.seed(0)\n",
        "  return np.random.randn(*shape)*0.5 #multiplied by 0.5 to have smaller values, to have better initialisation weights\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "    # Calculate the number of inputs and outputs\n",
        "    n_in = shape[0]\n",
        "    n_out = shape[1]    \n",
        "    # Calculate the variance of the normal distribution\n",
        "    variance = 2.0 / (n_in + n_out)\n",
        "    # Initialize the weights with random values drawn from the normal distribution\n",
        "    np.random.seed(0)\n",
        "    weights = np.random.randn(n_in, n_out) * np.sqrt(variance)*2.0\n",
        "    \n",
        "    return weights\n",
        "\n",
        "#------------------------------------------------------------------INITIALIZING WEIGHTS AND BIASES------------------------------------------------------------------------------------\n",
        "def initialize_w_b(input_layer,hidden_layer,output_layer,init):\n",
        "  weights=[]\n",
        "  biases=[]\n",
        "  layers=[input_layer]+hidden_layer+[output_layer]\n",
        "  for i in range(len(hidden_layer)+1):\n",
        "     if init=='random':\n",
        "       weights.append(random_initialisation((layers[i+1],layers[i])))\n",
        "     if init=='xavier':\n",
        "       weights.append(xavier_initialisation((layers[i+1],layers[i])))\n",
        "     biases.append(np.random.randn(layers[i+1],1))\n",
        "  return weights, biases\n",
        "\n",
        "'''\n",
        "#testing weights and biases output(used for debugging shape size of matrix) \n",
        "output_layer = 10\n",
        "input_layer = X_test.shape[1]\n",
        "print(X_test.shape)\n",
        "hidden_layer = [64, 64]\n",
        "weights, biases = initialize_w_b(input_layer, hidden_layer, output_layer, 'xavier')\n",
        "for i in range(len(weights)):\n",
        "    print(weights[i].shape)\n",
        "    \n",
        "for i in range(len(biases)):\n",
        "    print(biases[i].shape)\n",
        "\n",
        "weights2, biases2 = initialize_w_b(input_layer, hidden_layer, output_layer, 'random')\n",
        "for i in range(len(weights2)):\n",
        "    print(weights2[i].shape)\n",
        "    \n",
        "for i in range(len(biases2)):\n",
        "    print(biases2[i].shape)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "SIulf2iI377n",
        "outputId": "51974b70-0268-4638-c417-dfc388e8cda9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#testing weights and biases output(used for debugging shape size of matrix) \\noutput_layer = 10\\ninput_layer = X_test.shape[1]\\nprint(X_test.shape)\\nhidden_layer = [64, 64]\\nweights, biases = initialize_w_b(input_layer, hidden_layer, output_layer, 'xavier')\\nfor i in range(len(weights)):\\n    print(weights[i].shape)\\n    \\nfor i in range(len(biases)):\\n    print(biases[i].shape)\\n\\nweights2, biases2 = initialize_w_b(input_layer, hidden_layer, output_layer, 'random')\\nfor i in range(len(weights2)):\\n    print(weights2[i].shape)\\n    \\nfor i in range(len(biases2)):\\n    print(biases2[i].shape)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network**\n",
        "1. With forward propagation\n",
        "2. Backward propagation with different optimizers\n",
        "3. A train function, which fits and trains the given set"
      ],
      "metadata": {
        "id": "L20aDODvd-I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet():\n",
        "\n",
        "  def __init__(self,input_layer,hidden_layer,output_layer,initialisation_func,act_function,loss_func):\n",
        "     self.input_layer=input_layer\n",
        "     self.hidden_layer=hidden_layer\n",
        "     self.output_layer=output_layer\n",
        "     self.initialisation_func=initialisation_func\n",
        "     self.act_function=act_function\n",
        "     self.loss_func=loss_func\n",
        "     self.weights,self.biases = initialize_w_b(self.input_layer,self.hidden_layer,self.output_layer,self.initialisation_func)\n",
        "     self.layer_size=len(self.hidden_layer)\n",
        "\n",
        "  def forward_propagation(self,X):\n",
        "     #pre-activation\n",
        "     self.a=[]\n",
        "     #post-activation\n",
        "     self.h=[]\n",
        "     l=0\n",
        "     \n",
        "     #pre-activation and post-activation for input layer and first hidden layer\n",
        "     self.a.append((self.weights[l]@X)+self.biases[l])#WX+b\n",
        "     if self.act_function=='sigmoid':\n",
        "       self.h.append(sigmoid(self.a[l]))\n",
        "     elif self.act_function=='relu':\n",
        "       self.h.append(relu(self.a[l]))\n",
        "     elif self.act_function=='tanh':\n",
        "       self.h.append(tanh(self.a[l]))\n",
        "     \n",
        "     #pre-activation and post-activation between hidden layers\n",
        "     for l in range(1,self.layer_size):\n",
        "       self.a.append((self.weights[l]@self.h[l-1])+self.biases[l]) #W.X+b\n",
        "       if self.act_function=='sigmoid':\n",
        "        self.h.append(sigmoid(self.a[l]))\n",
        "       elif self.act_function=='relu':\n",
        "        self.h.append(relu(self.a[l]))\n",
        "       elif self.act_function=='tanh':\n",
        "        self.h.append(tanh(self.a[l]))\n",
        "       \n",
        "     #pre-activation and post-activation between last hidden layer and output layer\n",
        "     l=self.layer_size \n",
        "     self.a.append((self.weights[l]@self.h[l-1])+self.biases[l]) #WX+b\n",
        "     self.h.append(softmax(self.a[l]))\n",
        "     \n",
        "     #we only need the output of the end layer\n",
        "     return self.h[-1]\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------Q3 BACK PROPPAGATION FRAMEWORK WITH OPTIMIZATION FUNCTIONS-----------------------------------------------------------------\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
        "  def back_propagation(self,X,Y):\n",
        "\n",
        "        g_a  = [0]*(self.layer_size+1)\n",
        "        g_h  = [0]*(self.layer_size+1)\n",
        "        g_w  = [0]*(len(self.weights))\n",
        "        g_b  = [0]*(len(self.biases))\n",
        "        #Due to error accumulation the gradients values increases, so we average the value with batch size \n",
        "        batch_size = X.shape[1] \n",
        "\n",
        "        for k in reversed(range(self.layer_size+1)):\n",
        "            #calculating loss function for weights and biases at output\n",
        "            if k == self.layer_size:\n",
        "              if self.loss_func == 'cross_entropy':\n",
        "                  g_a[k] = self.h[k]  - Y  \n",
        "              elif self.loss_func == 'square_loss': \n",
        "                  g_a[k] = (self.h[k] - Y) * self.h[k] * (1 - self.h[k]) \n",
        "            \n",
        "            #calculating gradients for hidden layers     \n",
        "            else:\n",
        "                g_h[k] = (1/batch_size)*((self.weights[k+1].T)@(g_a[k+1]))\n",
        "                if self.act_function == 'sigmoid':\n",
        "                  g_a[k] = (1/batch_size)*((g_h[k])*(grad_sigmoid(self.a[k])))#Here we use '*' operator for elementwise matrix multiplication\n",
        "                elif self.act_function == 'tanh':\n",
        "                  g_a[k] = (1/batch_size)*((g_h[k])*(grad_tanh(self.a[k])))#Here we use '*' operator for elementwise matrix multiplication\n",
        "                elif self.act_function == 'relu':\n",
        "                  g_a[k] = (1/batch_size)*((g_h[k])*(grad_relu(self.a[k])))#Here we use '*' operator for elementwise matrix multiplication\n",
        "                \n",
        "            #calculating gradients of weights \n",
        "            if k == 0:\n",
        "                g_w[k] = (1/batch_size)*((g_a[k])@(X.T)) \n",
        "            else:\n",
        "                g_w[k] = (1/batch_size)*((g_a[k])@(self.h[k-1].T))\n",
        "            \n",
        "            #calculating gradients of biases\n",
        "            g_b[k]  = (1/batch_size)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_w,g_b\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#-----------------------------------------------------STOCHASTIC GRADIENT DESCENT AND OTHER OPTIMIZERS-------------------------------------------------------------------------------\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
        "  def stochastic_gradient_descent(self,W,b,g_w,g_b,lr,lambd):\n",
        "      #Weights=Weights-(learning rate)*(grad weights)-(learning rate*lambda)*(Weights)(this is weight decay with L2 regularization)\n",
        "      W=W-np.multiply(lr,g_w)-np.multiply(lr*lambd,W)\n",
        "\n",
        "      #biases=biases-(learning rate)*(grad biases)\n",
        "      b=b-np.multiply(lr,g_b)\n",
        "      return W,b\n",
        "  \n",
        "  def momentum_based_gradient_descent(self,W,b,g_w,g_b, u_w_i, u_b_i, lr, lambd, gamma):\n",
        "      #u_t=(beta)*(u_t-1)+(grad weight)[u_0 is initialised as zero]\n",
        "      u_w=np.multiply(gamma,u_w_i)+g_w\n",
        "      #weights=weights-(lr)*(updated_weights)-(weight decay L2 regularization)\n",
        "      W=W-np.multiply(lr,u_w)-np.multiply(lr*lambd,W)\n",
        "      \n",
        "      #similarly for biases but without weight decay term\n",
        "      u_b=np.multiply(gamma,u_b_i)+g_b\n",
        "      b=b-np.multiply(lr,u_b)\n",
        "      return W,b,u_w,u_b\n",
        "  \n",
        "  def nesterov_accelerated_gradient_descent(self,W,b,g_w,g_b,lr,lambd,gamma,u_w_i, u_b_i,b_input,b_output):\n",
        "      w_t=W\n",
        "      b_t=b\n",
        "      #here we make changes to global variables as we need to update the weights(look ahead) for calculating gradients\n",
        "      self.weights = self.weights - np.multiply(gamma,u_w_i)\n",
        "      self.biases = self.biases - np.multiply(gamma,u_b_i)\n",
        "      output =  self.forward_propagation(b_input)\n",
        "      #in this step, gradients are recalculated in global variables with updated weight values \n",
        "      g_weights,g_biases = self.back_propagation(b_input,b_output)\n",
        "\n",
        "      #u_t=(gamma)*(u_(t-1))+gradient of(w_t-(gamma)*(u_(t-1)))\n",
        "      u_w=np.multiply(gamma,u_w_i)+g_weights\n",
        "      #weights=weights-(lr)*(updated_weights)-(weight decay L2 regularization)\n",
        "      w_t = w_t - u_w - np.multiply(lr*lambd,w_t)\n",
        "\n",
        "      #similarly for biases but without weight decay term\n",
        "      u_b=np.multiply(gamma,u_b_i)+g_b\n",
        "      b=b-np.multiply(lr,u_b) \n",
        "\n",
        "      return w_t,b,u_w,u_b\n",
        "  \n",
        "  def rmsprop(self,W,b,g_w,g_b,lr,lambd,beta,eps,vw,vb):\n",
        "      #V_t= (beta)*(V_t-1)+(1-beta)*(grad weight)^2 \n",
        "      vw = np.multiply(vw,beta) + np.multiply(1-beta,np.power(g_w,2))           \n",
        "      #similarly for biases\n",
        "      vb = np.multiply(vb,beta) + np.multiply(1-beta,np.power(g_b,2))\n",
        "      \n",
        "      #w_t= (w_t-1)-((lr)/(V_t+eps)^(1/2))*(grad weight)-(L2 regularization weight decay)\n",
        "      W = W - np.multiply(g_w,lr/np.power(vw+eps,1/2))- np.multiply(lr*lambd,W)\n",
        "      #similarly for biases\n",
        "      b = b - np.multiply(g_b,lr/np.power(vb+eps,1/2))\n",
        "      return W,b,vw,vb\n",
        "  \n",
        "  def adam(self,W,b,g_w,g_b,beta1,beta2,lr ,m_t_i ,v_t_i ,m_b_i ,v_b_i,eps,i,lambd):\n",
        "      \n",
        "      m_t = np.multiply(beta1,m_t_i) + np.multiply(1-beta1,g_w)\n",
        "      v_t = np.multiply(beta2,v_t_i) + np.multiply(1-beta2,np.power(g_w,2))\n",
        "      m_b = np.multiply(beta1,m_b_i) + np.multiply(1-beta1,g_b)\n",
        "      v_b = np.multiply(beta2,v_b_i) + np.multiply(1-beta2,np.power(g_b,2))\n",
        "                \n",
        "      #normalization of moment          \n",
        "      m_hat_w = m_t/(1 - np.power(beta1,i+1))\n",
        "      m_hat_b = m_b/(1 - np.power(beta1,i+1))\n",
        "\n",
        "      #normalization          \n",
        "      v_hat_w = v_t/(1 - np.power(beta2,i+1))\n",
        "      v_hat_b = v_b/(1 - np.power(beta2,i+1))\n",
        "      \n",
        "      W = W - ((lr / np.power(v_hat_w + eps, 1/2)) * m_hat_w) - np.multiply(lr*lambd,W)\n",
        "      \n",
        "      b = b - ((lr / np.power(v_hat_b + eps, 1/2)) * m_hat_b)\n",
        "      return W,b,m_t,v_t,m_b,v_b\n",
        "  \n",
        "  def nadam(self,W,b,g_w,g_b,beta1,beta2,lr ,m_t_i ,v_t_i ,m_b_i ,v_b_i,eps,i,lambd):\n",
        "      m_t =  np.multiply(beta1,m_t_i) + np.multiply(1 - beta1,g_w)\n",
        "      v_t =  np.multiply(beta2,v_t_i) + np.multiply(1 - beta2,np.power(g_w, 2))\n",
        "\n",
        "      m_b =  np.multiply(beta1,m_b_i) + np.multiply(1 - beta1,g_b)\n",
        "      v_b =  np.multiply(beta2,v_b_i) + np.multiply(1 - beta2,np.power(g_b, 2))\n",
        "                \n",
        "      m_hat_w = m_t / (1 - np.power(beta1, i+1)) \n",
        "      v_hat_t = v_t / (1 - np.power(beta2, i+1))\n",
        "\n",
        "      m_hat_b = m_b / (1 - np.power(beta1, i+1)) \n",
        "      v_hat_b = v_b / (1 - np.power(beta2, i+1))\n",
        "  \n",
        "      a1 = (1-beta1)/(1-np.power(beta1,i+1))\n",
        "      update_w = np.multiply(lr/(np.power(v_hat_t + eps,1/2)),(np.multiply(a1,g_w) + np.multiply(beta1,m_hat_w)))\n",
        "      update_b = np.multiply(lr/(np.power(v_hat_b + eps,1/2)),(np.multiply(a1,g_b)+np.multiply(beta1,m_hat_b) ))\n",
        "      W = W - update_w - np.multiply(lr*lambd,W)\n",
        "      b = b - update_b    \n",
        "      return W,b,m_t,v_t,m_b,v_b\n",
        "  \n",
        "\n",
        "\n",
        "  def predict(self, X,y ):\n",
        "      output =  self.forward_propagation(X)\n",
        "      out_class=(np.argmax(output,axis=0))\n",
        "      accuracy = round(accuracy_score(y, out_class) * 100, 3)\n",
        "      return accuracy , out_class\n",
        "\n",
        "  def train(self,X_train,y_train,X_val ,y_val ,learning_rate,epochs, optimiser='gd',batch_size = 64,lambd=0.0005,WandB=False): \n",
        "\n",
        "      update_w = np.zeros(np.array(self.weights).shape)\n",
        "      update_b = np.zeros(np.array(self.biases).shape)\n",
        "      update_w_i = np.zeros(np.array(self.weights).shape)\n",
        "      update_b_i = np.zeros(np.array(self.biases).shape)\n",
        "      vw_i = 0.0\n",
        "      vb_i = 0.0\n",
        "      m_t_i=0\n",
        "      v_t_i=0\n",
        "      m_b_i=0\n",
        "      v_b_i=0\n",
        "      eps = 1e-8\n",
        "      gamma = 0.9\n",
        "      beta = 0.999\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      m_t, v_t, m_hat_w, v_hat_w, m_b,v_b,m_hat_b,v_hat_b = 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 \n",
        "\n",
        "      train_accuracy, val_accuracy, training_loss ,validation_loss = [] ,[] ,[] ,[]\n",
        "      for i in tqdm(range(epochs)):\n",
        "        for batch in range(0, X_train.shape[1], batch_size):\n",
        "\n",
        "          batch_images =  X_train[:,batch:batch+batch_size]\n",
        "          batch_output =  Y_train_encoded[:,batch:batch+batch_size]\n",
        "          output =  self.forward_propagation(batch_images)\n",
        "          g_weights,g_biases = self.back_propagation(batch_images,batch_output)\n",
        "          if optimiser == 'gd':\n",
        "              self.weights,self.biases=self.stochastic_gradient_descent(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd)\n",
        "          \n",
        "          if optimiser == 'mgd':\n",
        "              self.weights,self.biases,update_w,update_b=self.momentum_based_gradient_descent(self.weights,self.biases,g_weights,g_biases,update_w_i,update_b_i,learning_rate,lambd,gamma)\n",
        "              update_w_i = update_w\n",
        "              update_b_i = update_b\n",
        "\n",
        "          if optimiser == 'ngd':\n",
        "              w_t,b_t,update_w,update_b=self.nesterov_accelerated_gradient_descent(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd,gamma,update_w_i,update_b_i,batch_images,batch_output)\n",
        "              self.weights = w_t\n",
        "              self.biases = b_t\n",
        "              update_w_i = update_w\n",
        "              update_b_i = update_b\n",
        "\n",
        "\n",
        "          if optimiser == 'rmsprop': \n",
        "              self.weights,self.biases,vw,vb= self.rmsprop(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd,beta,eps,vw_i,vb_i)\n",
        "              vw_i=vw\n",
        "              vb_i=vb\n",
        "\n",
        "          if optimiser == 'adam':\n",
        "              self.weights,self.biases,m_t,v_t,m_b,v_b=self.adam(self.weights,self.biases,g_weights,g_biases,beta1,beta2,learning_rate, m_t_i, v_t_i, m_b_i, v_b_i,eps,i,lambd)\n",
        "              m_t_i=m_t\n",
        "              v_t_i=v_t\n",
        "              m_b_i=m_b\n",
        "              v_b_i=v_b\n",
        "\n",
        "          \n",
        "          if optimiser == 'nadam':\n",
        "              self.weights,self.biases,m_t,v_t,m_b,v_b=self.nadam(self.weights,self.biases,g_weights,g_biases,beta1,beta2,learning_rate, m_t_i,v_t_i, m_b_i,v_b_i, eps, i, lambd)\n",
        "              m_t_i=m_t\n",
        "              v_t_i=v_t\n",
        "              m_b_i=m_b\n",
        "              v_b_i=v_b                         \n",
        "\n",
        "\n",
        "        #Training loss for full dataset\n",
        "        predicted_train = self.forward_propagation(X_train)\n",
        "        predicted_train_label=(np.argmax(predicted_train,axis=0)) \n",
        "        acc1 = 100*np.sum(predicted_train_label==y_train)/predicted_train.shape[1]\n",
        "        train_accuracy.append(acc1)\n",
        "\n",
        "\n",
        "        predicted_val = self.forward_propagation(X_val)\n",
        "        predicted_val_label=(np.argmax(predicted_val,axis=0))\n",
        "        acc2 = 100*np.sum(predicted_val_label==y_val)/predicted_val.shape[1]   \n",
        "        val_accuracy.append(acc2)\n",
        "\n",
        "        a =self.weights[1:len(self.hidden_layer)]\n",
        "        b = np.sum([(np.sum((a[i]**2).reshape(1,-1))) for i in range(len(a))]) \n",
        "        if self.loss_func == 'cross_entropy':\n",
        "          train_loss = (-np.sum(np.multiply(Y_train_encoded,np.log(predicted_train)))+((lambd/2.)*b))/y_train.shape[0]\n",
        "          val_loss = (-np.sum(np.multiply(Y_val_encoded,np.log(predicted_val)))+((lambd/2.)*b))/y_val.shape[0]\n",
        "\n",
        "        elif self.loss_func == 'square_loss':\n",
        "\n",
        "          val_loss =((1/2) * np.sum((Y_val_encoded- predicted_val)**2))/y_val.shape[0]\n",
        "          train_loss = (1/2) * np.sum((Y_train_encoded - predicted_train)**2)/y_train.shape[0]\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(val_loss)\n",
        "          \n",
        "\n",
        "        print('Epoch {} : training_accuracy = {:.2f}, training_loss = {:.5f},Validation accuracy = {:.2f},Validation loss = {:.5f}'.format(i+1,acc1,train_loss, acc2,val_loss))\n",
        "        if WandB==True:\n",
        "          wandb.log({\"val_accuracy\": acc2,\"accuracy\": acc1,\"steps\":epochs,\"train_loss\":train_loss,\"val_loss\":val_loss},)\n",
        "        \n",
        "      \n",
        "      return train_accuracy,val_accuracy,training_loss, validation_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "4QHPBlX6Q5gF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 Forward propagation solved"
      ],
      "metadata": {
        "id": "uBo3daWQqabz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZFg5zjTIG41z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_class = 10\n",
        "\n",
        "model = NeuralNet(28*28,[128,128,128],n_class, initialisation_func = 'xavier', act_function='tanh', loss_func = 'cross_entropy')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "acc1,acc2,train_loss, val_loss= model.train(X_train,Y_train,X_val ,Y_val ,learning_rate = 0.01,epochs=5, optimiser='adam',batch_size =32,lambd=0.0005)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVHUzQYN7TOu",
        "outputId": "4046ebc8-17b0-416d-923c-d2ee981704b5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:15<01:03, 15.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 : training_accuracy = 80.00, training_loss = 0.54152,Validation accuracy = 80.10,Validation loss = 0.55204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:32<00:48, 16.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 : training_accuracy = 81.66, training_loss = 0.49802,Validation accuracy = 81.00,Validation loss = 0.51371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:47<00:31, 15.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 : training_accuracy = 82.71, training_loss = 0.47132,Validation accuracy = 82.22,Validation loss = 0.49101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [01:02<00:15, 15.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 : training_accuracy = 83.18, training_loss = 0.45617,Validation accuracy = 82.75,Validation loss = 0.47944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [01:17<00:00, 15.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 : training_accuracy = 83.62, training_loss = 0.44449,Validation accuracy = 82.97,Validation loss = 0.47084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q1    = NeuralNet(X_train.shape[0],[64,64,64], 10,'xavier','sigmoid','cross_entropy')\n",
        "A=q1.forward_propagation(X_train)\n",
        "A=A.T\n",
        "print(A[0])\n",
        "print(X_train.shape[1])\n",
        "print(Y_train[0])\n",
        "print((Y_train_encoded.T)[0])"
      ],
      "metadata": {
        "id": "RPh4aKLwnXfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Fj2Hcd953Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random,bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'  \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [15,20,30]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.001,0.0001]\n",
        "        },\n",
        "        'loss_function':{\n",
        "            'values':['cross_entropy']\n",
        "        },\n",
        "        'initilisation':{\n",
        "            'values':['random','xavier']\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[32,64,128]\n",
        "        },\n",
        "        'optimiser': {\n",
        "            'values': ['gd','mgd','ngd', 'rmsprop','adam','nadam']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu','tanh','sigmoid']\n",
        "        },\n",
        "        'hidden_layer': {\n",
        "            'values': [\n",
        "                       [64,64,64],\n",
        "                       [64,64,64,64],[64,64,64,64,64],[128,128,128],\n",
        "                       [128,128,128,128],[128,128,128,128,128],[256,256,256]]\n",
        "        },\n",
        "        \n",
        "        'weight_decay':{\n",
        "            'values':[0.0,0.0005]\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity='shreyashgadgil007', project=\"CS-6910 A1\")\n",
        "\n",
        "def train1():\n",
        "  steps = 10\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults = {\n",
        "      'epochs': 5,\n",
        "      'input_layer': 784,\n",
        "      'output_layer': 10,\n",
        "      'batch_size':64,\n",
        "      'weight_decay':0.005,\n",
        "      'learning_rate': 0.0001,\n",
        "      'hidden_layer':[16,16,16],\n",
        "      'optimiser':'mgd',\n",
        "      'activation':'sigmoid',\n",
        "      'initialisation':'xavier',\n",
        "      'loss_function':'cross_entropy'\n",
        "      \n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  wandb.init(project='CS-6910 A1', entity='shreyashgadgil007',config=config_defaults)\n",
        "  wandb.run.name = 'b_s:'+str(wandb.config.batch_size)+',lr:'+ str(wandb.config.learning_rate)+',ep:'+str(wandb.config.epochs)+ ',opt:'+str(wandb.config.optimiser)+ ',hl:'+str(wandb.config.hidden_layer)+ ',act:'+str(wandb.config.activation)+',decay:'+str(wandb.config.weight_decay)+',init:'+str(wandb.config.initialisation)+',loss:'+str(wandb.config.loss_function)\n",
        "\n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "  learning_rate = config.learning_rate\n",
        "  epochs = int(config.epochs)\n",
        "  hidden_layer = config.hidden_layer\n",
        "  activation = config.activation\n",
        "  optimiser = config.optimiser\n",
        "  input_layer = config.input_layer\n",
        "  output_layer = config.output_layer\n",
        "  batch_size = config.batch_size\n",
        "  weight_decay = config.weight_decay\n",
        "  loss_function = config.loss_function\n",
        "  initialisation = config.initilisation\n",
        "  # Model training here\n",
        "  sweep_network    = NeuralNet(input_layer, hidden_layer, output_layer,initialisation,activation,loss_function)\n",
        "  acc1,acc2,train_loss,val_loss  = sweep_network.train(X_train,Y_train,X_val,Y_val,learning_rate,epochs,optimiser,batch_size,weight_decay)\n",
        "  # print('Acc',acc2)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "wandb.agent(sweep_id, function=train1, count=120)\n"
      ],
      "metadata": {
        "id": "VEWqM8s_YY1C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}